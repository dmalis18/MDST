{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Checkpoint:</h1>\n",
    "\n",
    "**Looking to see completetion and effort in completing the checkpoint. It's okay if it's not correct**\n",
    "\n",
    "Based off this dataset with school financial, enrollment, and achievement data, we are interested in what information is a useful indicator of student performance at the state level.\n",
    "\n",
    "This question is a bit too big for a checkpoint, however. Instead, we want you to look at smaller questions related to our overall goal. Here's the overview:\n",
    "\n",
    "1. Choose a specific test to focus on\n",
    ">Math/Reading for 4/8 grade\n",
    "* Pick or create features to use\n",
    ">Will all the features be useful in predicting test score? Are some more important than others? Should you standardize, bin, or scale the data?\n",
    "* Explore the data as it relates to that test\n",
    ">Create 2 well-labeled visualizations (graphs), each with a caption describing the graph and what it tells us about the data\n",
    "* Create training and testing data\n",
    ">Do you want to train on all the data? Only data from the last 10 years? Only Michigan data?\n",
    "* Train a ML model to predict outcome \n",
    ">Pick if you want to do a regression or classification task. For both cases, defined _exactly_ what you want to predict, and pick any model in sklearn to use (see sklearn <a href=\"https://scikit-learn.org/stable/modules/linear_model.html\">regressors</a> and <a href=\"https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\">classifiers</a>).\n",
    "* Summarize your findings\n",
    ">Write a 1 paragraph summary of what you did and make a recommendation about if and how student performance can be predicted\n",
    "\n",
    "** Include comments throughout your code! Every cleanup and preprocessing task should be documented.\n",
    "\n",
    "\n",
    "Of course, if you're finding this assignment interesting (and we really hope you do!), you are welcome to do more than the requirements! For example, you may want to see if expenditure affects 4th graders more than 8th graders. Maybe you want to look into the extended version of this dataset and see how factors like sex and race are involved. You can include all your work in this notebook when you turn it in -- just always make sure you explain what you did and interpret your results. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Cleanup </h2>\n",
    "\n",
    "Import numpy, pandas, matplotlib, and seaborn\n",
    "\n",
    "(Feel free to import other libraries!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mat\n",
    "import seaborn as sea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the \"states_edu.csv\" dataset and take a look at the head of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/states_edu.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always familiarize yourself with what each column in the dataframe represents. \\ Read about the states_edu dataset here: https://www.kaggle.com/noriuk/us-education-datasets-unification-project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this space to rename columns, deal with missing data, etc. _(optional)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       252.0\n",
       "1         NaN\n",
       "2       265.0\n",
       "3       256.0\n",
       "4       261.0\n",
       "        ...  \n",
       "1710    287.0\n",
       "1711    286.0\n",
       "1712    272.0\n",
       "1713    289.0\n",
       "1714    286.0\n",
       "Name: AVG_MATH_8_SCORE, Length: 1715, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"AVG_MATH_8_SCORE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exploratory Data Analysis (EDA) </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chosen Predictor for Test: **Math for 8th grade**   (Ex. Math for 8th grade)\n",
    "\n",
    "**(hit `Enter` to edit)**\n",
    "\n",
    "Predictor Score in the questions refers to the predictor variable you chose here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many different years of data are in our dataset? Use a pandas function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.value_counts(\"YEAR\")\n",
    "len(df2) #Gives 33 \n",
    "\n",
    "df[\"YEAR\"].nunique() #Also gives 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare Michigan to Ohio. Which state has the higher average predictor score across all years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MICHIGAN MEAN: 276.1666666666667\n",
      "OHIO MEAN: 282.25\n",
      "OHIO has the higher average predictor score across all years\n"
     ]
    }
   ],
   "source": [
    "mich_mean = df[df[\"STATE\"]==\"MICHIGAN\"][\"AVG_MATH_8_SCORE\"].mean()\n",
    "ohio_mean = df[df[\"STATE\"]==\"OHIO\"][\"AVG_MATH_8_SCORE\"].mean()\n",
    "print(\"MICHIGAN MEAN: \" + str(mich_mean) + \"\\nOHIO MEAN: \" + str(ohio_mean))\n",
    "print(\"OHIO has the higher average predictor score across all years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the average for your predictor score across all states in 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278.28073089700996"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"AVG_MATH_8_SCORE\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the maximum predictor score for every state. Hint: there's a function that allows you to do this easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ALABAMA': 269.0,\n",
       " 'ARIZONA': 283.0,\n",
       " 'ARKANSAS': 279.0,\n",
       " 'CALIFORNIA': 277.0,\n",
       " 'COLORADO': 292.0,\n",
       " 'CONNECTICUT': 289.0,\n",
       " 'DELAWARE': 284.0,\n",
       " 'DISTRICT_OF_COLUMBIA': 269.0,\n",
       " 'FLORIDA': 281.0,\n",
       " 'GEORGIA': 281.0,\n",
       " 'HAWAII': 281.0,\n",
       " 'IDAHO': 287.0,\n",
       " 'INDIANA': 288.0,\n",
       " 'IOWA': 286.0,\n",
       " 'KENTUCKY': 282.0,\n",
       " 'LOUISIANA': 273.0,\n",
       " 'MAINE': 289.0,\n",
       " 'MARYLAND': 288.0,\n",
       " 'MASSACHUSETTS': 301.0,\n",
       " 'MICHIGAN': 280.0,\n",
       " 'MINNESOTA': 295.0,\n",
       " 'MISSISSIPPI': 274.0,\n",
       " 'MISSOURI': 286.0,\n",
       " 'NEBRASKA': 288.0,\n",
       " 'NEW_HAMPSHIRE': 296.0,\n",
       " 'NEW_JERSEY': 296.0,\n",
       " 'NEW_MEXICO': 274.0,\n",
       " 'NEW_YORK': 283.0,\n",
       " 'NORTH_CAROLINA': 286.0,\n",
       " 'NORTH_DAKOTA': 293.0,\n",
       " 'OHIO': 290.0,\n",
       " 'OKLAHOMA': 279.0,\n",
       " 'PENNSYLVANIA': 290.0,\n",
       " 'RHODE_ISLAND': 284.0,\n",
       " 'SOUTH_CAROLINA': 282.0,\n",
       " 'TENNESSEE': 280.0,\n",
       " 'TEXAS': 290.0,\n",
       " 'UTAH': 287.0,\n",
       " 'VIRGINIA': 290.0,\n",
       " 'WEST_VIRGINIA': 274.0,\n",
       " 'WISCONSIN': 289.0,\n",
       " 'WYOMING': 289.0,\n",
       " 'ILLINOIS': 285.0,\n",
       " 'KANSAS': 290.0,\n",
       " 'MONTANA': 293.0,\n",
       " 'NEVADA': 278.0,\n",
       " 'OREGON': 285.0,\n",
       " 'VERMONT': 295.0,\n",
       " 'ALASKA': 283.0,\n",
       " 'SOUTH_DAKOTA': 291.0,\n",
       " 'WASHINGTON': 290.0,\n",
       " 'NATIONAL': 285.0,\n",
       " 'DODEA': 293.0}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {}\n",
    "for i in range(len(df)):\n",
    "    if (df[\"STATE\"][i] in dict):\n",
    "        if (dict[df[\"STATE\"][i]] < df[\"AVG_MATH_8_SCORE\"][i]):\n",
    "            dict[df[\"STATE\"][i]] = df[\"AVG_MATH_8_SCORE\"][i]\n",
    "    elif (df[\"AVG_MATH_8_SCORE\"][i] > 0):\n",
    "            dict[df[\"STATE\"][i]] = df[\"AVG_MATH_8_SCORE\"][i]\n",
    "dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Feature Selection </h2>\n",
    "\n",
    "After exploring the data, you now have to choose features that you would use to predict the performance of the students on a chosen test (your chosen predictor). By the way, you can also create your own features. For example, perhaps you figured that maybe a state's expenditure per student may affect their overall academic performance so you create a expenditure_per_student feature.\n",
    "\n",
    "Use this space to modify or create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final feature list: **<LIST FEATURES HERE\\>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection justification: **<BRIEFLY DESCRIBE WHY YOU PICKED THESE FEATURES\\>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Visualization</h2>\n",
    "\n",
    "Use any graph you wish to see the relationship of your chosen predictor with any features you chose\n",
    "\n",
    "**Visualization 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<CAPTION FOR VIZ 1>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<CAPTION FOR VIZ 2>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Creation </h2>\n",
    "\n",
    "_Use this space to create train/test data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-c1333de87056>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-c1333de87056>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    X = ??\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "X = ??\n",
    "y = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=??, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prediction </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Models Resource: https://medium.com/@vijaya.beeravalli/comparison-of-machine-learning-classification-models-for-credit-card-default-data-c3cf805c9a5a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chosen ML task: **<REGRESSION/CLASSIFICATION>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import your sklearn class here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your model here\n",
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR CLASSIFICATION ONLY:\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(model, X_test, y_test,\n",
    "                         cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR REGRESSION ONLY: (pick a single column to visualize results)\n",
    "\n",
    "# Results from this graph _should not_ be used as a part of your results -- it is just here to help with intuition. \n",
    "# Instead, look at the error values and individual intercepts.\n",
    "\n",
    "\n",
    "col_name = ??\n",
    "col_index = X_train.columns.get_loc(col_name)\n",
    "\n",
    "f = plt.figure(figsize=(12,6))\n",
    "plt.scatter(X_train[col_name], y_train, color = \"red\")\n",
    "plt.scatter(X_train[col_name], model.predict(X_train), color = \"green\")\n",
    "plt.scatter(X_test[col_name], model.predict(X_test), color = \"blue\")\n",
    "\n",
    "new_x = np.linspace(X_train[col_name].min(),X_train[col_name].max(),200)\n",
    "intercept = model.predict([X_train.sort_values(col_name).iloc[0]]) - X_train[col_name].min()*model.coef_[col_index]\n",
    "plt.plot(new_x, intercept+new_x*model.coef_[col_index])\n",
    "\n",
    "plt.legend(['controlled model','true training','predicted training','predicted testing'])\n",
    "plt.xlabel(col_name)\n",
    "plt.ylabel(??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Summary </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<WRITE A PARAGRAPH SUMMARIZING YOUR WORK AND FINDINGS\\>**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
